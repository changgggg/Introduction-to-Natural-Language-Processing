{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a19e5f5-720c-4d84-8359-76fa5157238b",
      "metadata": {
        "id": "8a19e5f5-720c-4d84-8359-76fa5157238b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import BertTokenizer\n",
        "import sentencepiece as spm\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load IMDb dataset using the datasets library\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "train_texts = dataset['train']['text']\n",
        "train_labels = dataset['train']['label']\n",
        "test_texts = dataset['test']['text']\n",
        "test_labels = dataset['test']['label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec0fe1c-92ff-4982-b262-0257572c29f1",
      "metadata": {
        "id": "7ec0fe1c-92ff-4982-b262-0257572c29f1",
        "outputId": "3b7df351-9268-4415-bc47-3712486366aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: train_texts.txt\n",
            "  input_format: \n",
            "  model_prefix: bpe\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 10000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: train_texts.txt\n",
            "trainer_interface.cc(380) LOG(WARNING) Found too long line (4195 > 4192).\n",
            "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 24326 sentences\n",
            "trainer_interface.cc(416) LOG(INFO) Skipped 1 too long sentences.\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=29727647\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9565% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=76\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999565\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 24326 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=14825189\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 230594 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 24326\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 260774\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 260774 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=92197 obj=11.1289 num_tokens=615728 num_tokens/piece=6.6784\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=80821 obj=8.81992 num_tokens=620639 num_tokens/piece=7.67918\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=60610 obj=8.79362 num_tokens=644382 num_tokens/piece=10.6316\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=60583 obj=8.78373 num_tokens=646416 num_tokens/piece=10.6699\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=45436 obj=8.84217 num_tokens=683974 num_tokens/piece=15.0536\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45436 obj=8.82932 num_tokens=683918 num_tokens/piece=15.0523\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34077 obj=8.91533 num_tokens=727534 num_tokens/piece=21.3497\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=34077 obj=8.89932 num_tokens=727457 num_tokens/piece=21.3474\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25557 obj=9.01705 num_tokens=774642 num_tokens/piece=30.3104\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25557 obj=8.9966 num_tokens=774632 num_tokens/piece=30.31\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19167 obj=9.13746 num_tokens=822481 num_tokens/piece=42.9113\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19167 obj=9.11222 num_tokens=822324 num_tokens/piece=42.9031\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14375 obj=9.28548 num_tokens=870699 num_tokens/piece=60.5704\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14375 obj=9.25539 num_tokens=870659 num_tokens/piece=60.5676\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11000 obj=9.44194 num_tokens=915429 num_tokens/piece=83.2208\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11000 obj=9.41023 num_tokens=915382 num_tokens/piece=83.2165\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: bpe.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: bpe.vocab\n",
            "/Users/longnguyen/anaconda3/envs/NLP/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define maximum sequence length\n",
        "max_length = 512\n",
        "max_sentence_length = 4192\n",
        "\n",
        "# Train a BPE tokenizer using SentencePiece\n",
        "with open('train_texts.txt', 'w') as f:\n",
        "    for text in train_texts:\n",
        "        if len(text) <= max_sentence_length:\n",
        "            f.write(\"%s\\n\" % text)\n",
        "\n",
        "spm.SentencePieceTrainer.train(input='train_texts.txt', model_prefix='bpe', vocab_size=10000, max_sentence_length=max_sentence_length)\n",
        "sp_bpe = spm.SentencePieceProcessor(model_file='bpe.model')\n",
        "\n",
        "# Tokenize using BPE and truncate\n",
        "train_texts_bpe = [sp_bpe.encode_as_ids(text)[:max_length] for text in train_texts]\n",
        "test_texts_bpe = [sp_bpe.encode_as_ids(text)[:max_length] for text in test_texts]\n",
        "\n",
        "# Standard tokenization using BertTokenizer and truncate\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_texts_non_bpe = [tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True) for text in train_texts]\n",
        "test_texts_non_bpe = [tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True) for text in test_texts]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642d6982-e4a8-419a-9951-1474a81dbf01",
      "metadata": {
        "id": "642d6982-e4a8-419a-9951-1474a81dbf01"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return texts_padded, labels\n",
        "\n",
        "train_dataset_bpe = IMDBDataset(train_texts_bpe, train_labels)\n",
        "test_dataset_bpe = IMDBDataset(test_texts_bpe, test_labels)\n",
        "\n",
        "train_dataset_non_bpe = IMDBDataset(train_texts_non_bpe, train_labels)\n",
        "test_dataset_non_bpe = IMDBDataset(test_texts_non_bpe, test_labels)\n",
        "\n",
        "train_loader_bpe = DataLoader(train_dataset_bpe, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader_bpe = DataLoader(test_dataset_bpe, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "train_loader_non_bpe = DataLoader(train_dataset_non_bpe, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader_non_bpe = DataLoader(test_dataset_non_bpe, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf5cccf-34f6-4876-8b6a-a53ded8896cf",
      "metadata": {
        "id": "baf5cccf-34f6-4876-8b6a-a53ded8896cf"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        out = self.fc(lstm_out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2dbcb22-59eb-4991-8db3-b3cb1915eaef",
      "metadata": {
        "id": "b2dbcb22-59eb-4991-8db3-b3cb1915eaef",
        "outputId": "bab45044-e16e-4c58-d837-49e0b3b61b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BPE Model Accuracy: 0.50424\n",
            "Non-BPE Model Accuracy: 0.50144\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in test_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return accuracy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate and train models\n",
        "vocab_size_bpe = 10000\n",
        "vocab_size_non_bpe = tokenizer.vocab_size\n",
        "embed_dim = 128\n",
        "hidden_dim = 64\n",
        "output_dim = 2\n",
        "\n",
        "model_bpe = SentimentClassifier(vocab_size_bpe, embed_dim, hidden_dim, output_dim).to(device)\n",
        "optimizer_bpe = optim.Adam(model_bpe.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(model_bpe, train_loader_bpe, criterion, optimizer_bpe)\n",
        "accuracy_bpe = evaluate_model(model_bpe, test_loader_bpe)\n",
        "\n",
        "model_non_bpe = SentimentClassifier(vocab_size_non_bpe, embed_dim, hidden_dim, output_dim).to(device)\n",
        "optimizer_non_bpe = optim.Adam(model_non_bpe.parameters(), lr=0.001)\n",
        "\n",
        "train_model(model_non_bpe, train_loader_non_bpe, criterion, optimizer_non_bpe)\n",
        "accuracy_non_bpe = evaluate_model(model_non_bpe, test_loader_non_bpe)\n",
        "\n",
        "print(f\"BPE Model Accuracy: {accuracy_bpe}\")\n",
        "print(f\"Non-BPE Model Accuracy: {accuracy_non_bpe}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}